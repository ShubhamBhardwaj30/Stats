{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a8d2469",
   "metadata": {},
   "source": [
    "# üßº Data Preprocessing Toolbox with Examples\n",
    "This notebook contains key data preprocessing techniques every advanced data scientist should know ‚Äî with practical Python examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa07c8f",
   "metadata": {},
   "source": [
    "## üì¶ 1. Missing Value Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696fd158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "# Example dataset\n",
    "df = pd.DataFrame({\n",
    "    'A': [1, 2, np.nan, 4],\n",
    "    'B': [np.nan, 2, 3, 4]\n",
    "})\n",
    "\n",
    "# Simple imputation\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "# KNN imputation\n",
    "knn_imputer = KNNImputer(n_neighbors=2)\n",
    "df_knn = pd.DataFrame(knn_imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "df_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b14a5c",
   "metadata": {},
   "source": [
    "## üìè 2. Feature Scaling & Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3872d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "# Example dataset\n",
    "X = pd.DataFrame({'x1': [1, 10, 100, 1000], 'x2': [0.1, 0.5, 0.9, 0.95]})\n",
    "\n",
    "# Standard scaling\n",
    "standard = StandardScaler().fit_transform(X)\n",
    "\n",
    "# MinMax scaling\n",
    "minmax = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "# Robust scaling\n",
    "robust = RobustScaler().fit_transform(X)\n",
    "\n",
    "pd.DataFrame(robust, columns=['x1', 'x2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12f0033",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è 3. Encoding Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa18d102",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "df_cat = pd.DataFrame({'Color': ['Red', 'Blue', 'Green', 'Red']})\n",
    "\n",
    "# One-hot encoding\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoded = encoder.fit_transform(df_cat)\n",
    "pd.DataFrame(encoded, columns=encoder.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a164faf",
   "metadata": {},
   "source": [
    "## üîç 4. Outlier Detection & Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0926e048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "df_out = pd.DataFrame({'value': [10, 12, 13, 12, 100]})\n",
    "\n",
    "# Z-score method\n",
    "z_scores = stats.zscore(df_out)\n",
    "filtered = df_out[(abs(z_scores) < 2).all(axis=1)]\n",
    "filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30debb1",
   "metadata": {},
   "source": [
    "## üî¨ 5. Multicollinearity Detection with VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b98cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "df_vif = pd.DataFrame({\n",
    "    'X1': [1, 2, 3, 4, 5],\n",
    "    'X2': [2, 4, 6, 8, 10],  # Perfectly correlated with X1\n",
    "    'X3': [5, 3, 6, 2, 1]\n",
    "})\n",
    "\n",
    "# VIF calculation\n",
    "X_scaled = StandardScaler().fit_transform(df_vif)\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['feature'] = df_vif.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X_scaled, i) for i in range(X_scaled.shape[1])]\n",
    "vif_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfced29d",
   "metadata": {},
   "source": [
    "## üîÅ 6. Log Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185a9fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df_log = pd.DataFrame({'income': [500, 1000, 5000, 10000, 20000]})\n",
    "\n",
    "# Apply log transform\n",
    "df_log['log_income'] = np.log(df_log['income'])\n",
    "df_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1ebf05",
   "metadata": {},
   "source": [
    "## üéØ 7. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c00a21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Select top 2 features\n",
    "selector = SelectKBest(score_func=f_classif, k=2)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "X_new[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51665dd",
   "metadata": {},
   "source": [
    "## üß¨ 8. Dimensionality Reduction (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22927b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Standardize data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"PCA Result\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d4c4c8",
   "metadata": {},
   "source": [
    "## ü™µ 9. Binning / Discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91032488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "data = pd.DataFrame({'age': [22, 25, 47, 52, 46, 56]})\n",
    "binner = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n",
    "binned = binner.fit_transform(data)\n",
    "data['age_bin'] = binned\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233bf9be",
   "metadata": {},
   "source": [
    "## üîÑ 10. Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923419c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Pipeline with scaler + model\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}